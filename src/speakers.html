<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Speakers - ICIAP 2025 Breaking the Monolith Workshop</title>
    <link rel="stylesheet" href="css/styles.css">
    <style>
        .speakers-intro {
            margin-bottom: 40px;
            text-align: center;
        }
        
        .speaker-grid {
            /* display: grid; */
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
            gap: 40px;
            /* margin-top: 40px; */
        }
        
        .speaker-card {
            background-color: white;
            border-radius: 8px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
            overflow: hidden;
            transition: transform 0.3s;
            display: flex;
            flex-direction: column;
        }
        
        .speaker-card:hover {
            transform: translateY(-5px);
        }
        
        .speaker-top-row {
            display: flex;
            flex-direction: row;
            align-items: center;
        }
        
        .speaker-image {
            width: 25%;
            /* height: 250px; */
            object-fit: cover;
        }
        
        .speaker-info {
            padding: 20px;
            /* justify content */
            display: flex;
            flex-direction: column;
            justify-content: center;
            width: 75%;
        }
        
        .speaker-name {
            font-size: 22px;
            font-weight: bold;
            color: #0c4da2;
            margin-bottom: 5px;
        }
        
        .speaker-title {
            font-style: italic;
            color: #666;
            margin-bottom: 15px;
            font-size: 16px;
        }
        
        .speaker-bio {
            margin-bottom: 15px;
            line-height: 1.6;
            text-align: justify;
        }
        
        .talk-info {
            margin-top: 20px;
            padding: 15px 20px;
            border-top: 1px solid #eee;
        }
        
        .talk-title {
            font-weight: bold;
            color: #e63946;
            margin-bottom: 10px;
        }
        
        .talk-abstract {
            font-size: 14px;
            line-height: 1.6;
        }
    </style>
</head>
<body>
    <div id="header"></div>

    <!-- <section class="page-header">
        <div class="container">
            <h2>Keynote Speakers</h2>
        </div>
    </section> -->

    <section class="speakers">
        <div class="container">
            <div class="speakers-intro">
                <h2>Distinguished Speakers</h2>
                <p>We are honored to host leading experts in modular deep learning who will share their insights and latest research at the "Breaking the Monolith: 1st Workshop on Advances in Modular Deep Learning".</p>
            </div>
            
            <div class="speaker-grid">
                <div class="speaker-card">
                    <div class="speaker-top-row">
                        <img src="images/andrew_bagdanov.jpg" alt="Prof. Andrew David Bagdanov" class="speaker-image">
                        <div class="speaker-info">
                            <h3 class="speaker-name">Prof. Andrew David Bagdanov</h3>
                            <p class="speaker-title">Associate Professor, University of Florence, Italy</p>
                            <p class="speaker-bio">Andrew D. Bagdanov is currently Associate Professor at the University of Florence, Italy. He received his PhD in computer science in 2004 from the University of Amsterdam, after which he held a postdoctoral position at the University of Florence and a senior development position at the FAO of the United Nations. In 2013 he became Senior Researcher and Ramón y Cajal Fellow at the Computer Vision Center, Barcelona, and in 2015 began his tenure as Associate Professor at the University of Florence. He is the coordinator of the Master's Program in Artificial Intelligence at the University of Florence, and he leads a small research group whose research spans a broad spectrum of computer vision, image processing and deep learning.</p>
                        </div>
                    </div>
                    <div class="talk-info">
                        <h4 class="talk-title">How green is Modular Continual Learning, really?</h4>
                        <p class="talk-abstract">As research in Continual Learning continues gaining momentum, it is essential to understand its implications in the Foundation Model Era. The typically massive scale of foundation models demands efficient methods for continual adaptation, which has prompted a shift from monolithic approaches to more modular techniques such as prompt learning and adapter-based methods. While energy efficiency is often touted as an advantage of Continual Learning, a systematic comparative analysis of these methods regarding their efficiency-accuracy trade-offs is still missing in the literature. In this talk, I will explore the architectural differences between representative monolithic and modular approaches, and present an empirical evaluation of their energy consumption during training and inference. By uncovering the true energy costs of these methods, we hope to better assess their sustainability and impact on future AI research and practice.</p>
                        <p><strong>Date:</strong> September 16, 2025</p>
                    </div>
                </div>
                
                <div class="speaker-card">
                    <div class="speaker-top-row">
                        <img src="images/rodola.jpg" alt="Prof. Emanuele Rodolà" class="speaker-image">
                        <div class="speaker-info">
                            <h3 class="speaker-name">Prof. Emanuele Rodolà</h3>
                            <p class="speaker-title">Full Professor of Computer Science, Sapienza University of Rome, Italy</p>
                            <p class="speaker-bio">Emanuele Rodolà is a Full Professor of Computer Science at Sapienza University of Rome, where he leads the GLADIA AI group. His work in this field is supported by an ERC grant, a FIS grant, and a Google Research Award. In the past, he was a postdoctoral researcher at USI Lugano (2016–2017), an Alexander von Humboldt Fellow at TU Munich (2013–2016), and a JSPS Research Fellow at the University of Tokyo (2013), in addition to visiting periods at Tel Aviv University, Technion, École Polytechnique, and Stanford. He is a fellow of ELLIS and the only Italian AI researcher to be a fellow of the Young Academy of Europe. Professor Rodolà has received numerous awards for his research and plays an active role in the academic community, serving on program committees and as Area Chair for major conferences in AI and ML. His current research focuses primarily on neural model fusion, representation learning, ML for audio and music, and multimodal learning, with around 170 publications in these areas. His work has been featured in media outlets including Fortune, Wired, Italian national broadcast and newspapers.</p>
                        </div>
                    </div>
                    <div class="talk-info">
                        <h4 class="talk-title">Model Merging - What, Why, and How</h4>
                        <p class="talk-abstract">In this talk, I will introduce the emerging field of model merging — the process of combining multiple neural networks into a single model without retraining. We'll begin with foundational concepts such as linear mode connectivity and task vectors, and explore two main settings: (1) merging models trained from scratch on the same task but with different initializations, and (2) merging models finetuned on different tasks from a shared pretrained base. I will then present a series of recent works of mine that expand the model merging toolkit. These include the use of cycle consistency in permutation-based merging, insights into how task vectors relate to gradients, SVD-based approaches for low-rank model combination, and the application of evolutionary algorithms to discover optimal merging coefficients. Throughout, we'll see how these techniques can be applied in real-world scenarios, from model compression in Computer Vision to state-of-the-art synthesis of LLMs for low-resource languages.</p>
                        <p><strong>Date:</strong> September 16, 2025</p>
                    </div>
                </div>
                
                <div class="speaker-card">
                    <div class="speaker-top-row">
                        <img src="images/mbosc.png" alt="Dr. Matteo Boschini" class="speaker-image">
                        <div class="speaker-info">
                            <h3 class="speaker-name">Dr. Matteo Boschini</h3>
                            <p class="speaker-title">Head of AI, Symboolic S.r.l.</p>
                            <p class="speaker-bio">
                                Matteo Boschini received his Ph.D. degree in 2023 at the University of Modena and Reggio Emilia, Italy, within the AImageLab research group, working on machine learning, continual learning, and computer vision. He is currently Head of AI at Symboolic, working on applied generative language modelling.
                            </p>
                        </div>
                    </div>
                    <div class="talk-info">
                        <h4 class="talk-title">TBD</h4>
                        <!-- <p class="talk-abstract">TBD</p> -->
                        <p><strong>Date:</strong> September 16, 2025</p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <div id="footer"></div>

    <script src="js/main.js"></script>
</body>
</html>